#COMPAS Recidivism Dataset: Racial Bias Analysis with AI Fairness 360

#Overview
This notebook analyzes racial bias in risk scores generated by the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) system. Utilizing the COMPAS Recidivism Dataset and IBM's AI Fairness 360 toolkit, the primary goal is to identify and visualize disparities in false positive rates across different racial groups.

#Goal
Summarize the observed racial bias in risk scores, particularly regarding false positive rates, using the COMPAS Recidivism Dataset and AI Fairness 360, and present any insights gained from the analysis and visualizations.

#Dataset
COMPAS Recidivism Dataset: This dataset contains criminal defendant information, including their COMPAS risk scores and whether they re-offended within two years.

##Key Steps
**Load Dataset**: The compas-scores-two-years.csv file was loaded into a pandas DataFrame.
Preprocess Data for AIF360:

Filtered the dataset based on specific conditions (e.g., days_since_screening, c_charge_degree, two_year_recid, score_text).

Selected relevant features for analysis.

Handled missing values.

One-hot encoded categorical features (sex, race, age_cat, c_charge_degree).

Configure AIF360 Dataset Object:

An AIF360 StandardDataset and BinaryLabelDataset object was created.

race_African-American was designated as the protected attribute.

two_year_recid was set as the label column (0 for no recidivism, 1 for recidivism).

The decile_score was used as the risk score.

The 'Non-African-American' group (race_African-American = 0.0) was identified as the privileged group, and the 'African-American' group (race_African-American = 1.0) as the unprivileged group.

Analyze False Positive Rate Disparity:

Predicted labels were generated from the decile_score (threshold >= 5 for high risk).

ClassificationMetric from AIF360 was used to calculate False Positive Rates (FPR) for both privileged and unprivileged groups.

The False Positive Rate difference was also computed.

**Generate Disparity Visualization:**
A bar chart was generated using matplotlib and seaborn to visually represent the disparity in False Positive Rates between African-American and Non-African-American individuals.
**Key Findings**
False Positive Rate (African-American / Unprivileged Group): 0.3171

False Positive Rate (Non-African-American / Privileged Group): 0.5534

False Positive Rate Difference (Unprivileged - Privileged): -0.2363

**The analysis reveals a significant disparity**: the Non-African-American (Privileged) group exhibits a substantially higher False Positive Rate (0.5534) compared to the African-American (Unprivileged) group (0.3171). This means that individuals in the Non-African-American group are more often incorrectly classified as high risk for recidivism when they are not, compared to their African-American counterparts.

**Insights and Next Steps**
The observed disparity suggests that the COMPAS system may be over-predicting recidivism for Non-African-Americans more frequently than for African-Americans. This is a critical fairness concern, as it could lead to harsher legal consequences (e.g., stricter sentencing, increased supervision) for individuals who are not actually at high risk of re-offending.

Further investigation is crucial to understand the specific features and their interactions that contribute to this bias. This could involve feature importance analysis or exploring other fairness metrics.

To address this bias, implementing bias mitigation techniques is a necessary next step. Potential methods include reweighing the training data, applying adversarial debiasing, or post-processing adjustments to the model's predictions. The effectiveness of these techniques should be evaluated not only on fairness metrics but also on overall model performance to ensure fair and accurate outcomes.
